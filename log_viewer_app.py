#!/usr/bin/env python3
"""
FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ –∏ –æ—Ç—á–µ—Ç–æ–≤ Neural Deep Agent —Å–∏—Å—Ç–µ–º—ã.

–≠—Ç–æ—Ç —Å–µ—Ä–≤–µ—Ä –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:
- –õ–æ–≥–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤
- –û—Ç—á–µ—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
- Dashboard —Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
"""

import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import markdown
from pydantic import BaseModel

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI
app = FastAPI(
    title="SGR Deep Research Demo",
    description="Live capabilities showcase for Schema-Guided Reasoning framework with multi-model analysis",
    version="1.0.0"
)

# –ü—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
BASE_DIR = Path(__file__).parent
TEMPLATES_DIR = BASE_DIR / "templates"
SERVICES_DIR = BASE_DIR / "services"
LOGS_DIR = SERVICES_DIR / "logs"
REPORTS_DIR = SERVICES_DIR / "reports"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤
templates = Jinja2Templates(directory=str(TEMPLATES_DIR))

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
@dataclass
class LogFileInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –ª–æ–≥–∞ –∞–≥–µ–Ω—Ç–∞"""
    filename: str
    agent_id: str
    task: str
    iterations: int
    modified: datetime
    size: int
    model_name: str = "unknown"

@dataclass
class ReportInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –æ—Ç—á–µ—Ç–∞"""
    filename: str
    title: str
    description: str | None
    created: datetime
    size: int

class LogAnalytics(BaseModel):
    """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –ª–æ–≥—É"""
    total_steps: int
    reasoning_steps: int
    tool_executions: int
    search_operations: int
    total_duration: Optional[str] = None
    success_rate: float
    most_used_tools: List[Dict[str, Any]]
    model_name: Optional[str] = None

class ReportAnalytics(BaseModel):
    """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –æ—Ç—á–µ—Ç—É"""
    word_count: int
    sections: List[str]
    reading_time_minutes: int
    key_topics: List[str]


def parse_log_filename(filename: str) -> tuple[str, str, str]:
    """
    –ü–∞—Ä—Å–∏—Ç –∏–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    –§–æ—Ä–º–∞—Ç: YYYYMMDD-HHMMSS-sgr_agent_UUID-log.json
    """
    try:
        # –£–¥–∞–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        base_name = filename.replace('-log.json', '')
        parts = base_name.split('-')
        
        if len(parts) >= 4:
            date_str = parts[0]
            time_str = parts[1]
            agent_id = '-'.join(parts[3:])  # UUID –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–µ—Ñ–∏—Å—ã
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
            datetime_str = f"{date_str}-{time_str}"
            return datetime_str, agent_id, agent_id
        
        return filename, filename, "Unknown agent"
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–º—è —Ñ–∞–π–ª–∞ {filename}: {e}")
        return filename, filename, "Unknown agent"


def get_log_files() -> List[LogFileInfo]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤"""
    log_files = []
    
    if not LOGS_DIR.exists():
        return log_files
    
    for log_file in LOGS_DIR.glob("*.json"):
        try:
            datetime_str, agent_id, _ = parse_log_filename(log_file.name)
            
            # –ß–∏—Ç–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ñ–∞–π–ª–∞
            with open(log_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            task = log_data.get('task', 'Unknown task')
            iterations = len(log_data.get('log', []))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            model_info = log_data.get('model_info', {})
            model_name = model_info.get('model', 'gpt-4o-mini')
            
            log_info = LogFileInfo(
                filename=log_file.name,
                agent_id=agent_id,
                task=task,
                iterations=iterations,
                modified=datetime.fromtimestamp(log_file.stat().st_mtime),
                size=log_file.stat().st_size,
                model_name=model_name
            )
            log_files.append(log_info)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {log_file}: {e}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    log_files.sort(key=lambda x: x.modified, reverse=True)
    return log_files


def get_report_files() -> List[ReportInfo]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –æ—Ç—á–µ—Ç–æ–≤"""
    report_files = []
    
    if not REPORTS_DIR.exists():
        return report_files
    
    for report_file in REPORTS_DIR.glob("*.md"):
        try:
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è
            with open(report_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            lines = content.split('\n')
            title = report_file.stem
            description = None
            
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
            for i, line in enumerate(lines):
                if line.startswith('# '):
                    title = line[2:].strip()
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö
                    for j in range(i + 1, min(i + 10, len(lines))):
                        next_line = lines[j].strip()
                        if next_line and not next_line.startswith('*') and not next_line.startswith('#'):
                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–∏–µ
                            if len(next_line) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
                                description = next_line[:200] + ("..." if len(next_line) > 200 else "")
                                break
                    break
            
            report_info = ReportInfo(
                filename=report_file.name,
                title=title,
                description=description,
                created=datetime.fromtimestamp(report_file.stat().st_ctime),
                size=report_file.stat().st_size
            )
            report_files.append(report_info)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç—á–µ—Ç–∞ {report_file}: {e}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    report_files.sort(key=lambda x: x.created, reverse=True)
    return report_files


def analyze_log(log_data: Dict[str, Any]) -> LogAnalytics:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥ —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É"""
    log_entries = log_data.get('log', [])
    
    total_steps = len(log_entries)
    reasoning_steps = sum(1 for entry in log_entries if entry.get('step_type') == 'reasoning')
    tool_executions = sum(1 for entry in log_entries if entry.get('step_type') == 'tool_execution')
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
    search_operations = 0
    tool_usage = {}
    
    for entry in log_entries:
        if entry.get('step_type') == 'tool_execution':
            tool_name = entry.get('tool_name', 'unknown')
            tool_usage[tool_name] = tool_usage.get(tool_name, 0) + 1
            
            if 'search' in tool_name.lower() or 'tavily' in tool_name.lower():
                search_operations += 1
    
    # –¢–æ–ø –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    most_used_tools = [
        {"name": tool, "count": count} 
        for tool, count in sorted(tool_usage.items(), key=lambda x: x[1], reverse=True)[:5]
    ]
    
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
    task_completed = log_data.get('context', {}).get('state') == 'completed'
    success_rate = 1.0 if task_completed else 0.7  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    model_info = log_data.get('model_info', {})
    model_name = model_info.get('model', 'unknown')
    
    return LogAnalytics(
        total_steps=total_steps,
        reasoning_steps=reasoning_steps,
        tool_executions=tool_executions,
        search_operations=search_operations,
        success_rate=success_rate,
        most_used_tools=most_used_tools,
        model_name=model_name  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    )


def analyze_report(content: str) -> ReportAnalytics:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É"""
    # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤
    words = len(content.split())
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    sections = []
    for line in content.split('\n'):
        if line.startswith('#'):
            sections.append(line.strip())
    
    # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è (200 —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
    reading_time = max(1, words // 200)
    
    # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å NLP)
    key_topics = []
    common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'a', 'an'}
    
    words_freq = {}
    for word in content.lower().split():
        clean_word = ''.join(c for c in word if c.isalpha())
        if len(clean_word) > 4 and clean_word not in common_words:
            words_freq[clean_word] = words_freq.get(clean_word, 0) + 1
    
    key_topics = [word for word, freq in sorted(words_freq.items(), key=lambda x: x[1], reverse=True)[:10]]
    
    return ReportAnalytics(
        word_count=words,
        sections=sections,
        reading_time_minutes=reading_time,
        key_topics=key_topics
    )


# –ú–∞—Ä—à—Ä—É—Ç—ã API
@app.get("/", response_class=HTMLResponse)
async def dashboard(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å dashboard"""
    logs = get_log_files()
    reports = get_report_files()
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–ª—è –ø—Ä–µ–≤—å—é
    recent_logs = logs[:5]
    recent_reports = reports[:5]
    
    return templates.TemplateResponse("index.html", {
        "request": request,
        "logs": recent_logs,
        "reports": recent_reports
    })


@app.get("/logs", response_class=HTMLResponse)
async def logs_list(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤—Å–µ—Ö –ª–æ–≥–æ–≤"""
    logs = get_log_files()
    return templates.TemplateResponse("logs_list.html", {
        "request": request,
        "logs": logs
    })


@app.get("/reports", response_class=HTMLResponse)
async def reports_list(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –≤—Å–µ—Ö –æ—Ç—á–µ—Ç–æ–≤"""
    reports = get_report_files()
    return templates.TemplateResponse("reports_list.html", {
        "request": request,
        "reports": reports
    })


@app.get("/log/{filename}", response_class=HTMLResponse)
async def view_log(request: Request, filename: str):
    """–ü—Ä–æ—Å–º–æ—Ç—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ª–æ–≥–∞"""
    log_file = LOGS_DIR / filename
    
    if not log_file.exists():
        raise HTTPException(status_code=404, detail="–õ–æ–≥ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        return templates.TemplateResponse("log_view.html", {
            "request": request,
            "filename": filename,
            "log_data": log_data
        })
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –ª–æ–≥–∞ {filename}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –ª–æ–≥–∞")


@app.get("/report/{filename}", response_class=HTMLResponse)
async def view_report(request: Request, filename: str):
    """–ü—Ä–æ—Å–º–æ—Ç—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    report_file = REPORTS_DIR / filename
    
    if not report_file.exists():
        raise HTTPException(status_code=404, detail="–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(report_file, 'r', encoding='utf-8') as f:
            raw_content = f.read()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Markdown –≤ HTML
        html_content = markdown.markdown(
            raw_content,
            extensions=['tables', 'codehilite', 'toc']
        )
        
        return templates.TemplateResponse("report_view.html", {
            "request": request,
            "filename": filename,
            "content": html_content,
            "raw_content": raw_content
        })
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞ {filename}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞")


# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è JSON –¥–∞–Ω–Ω—ã—Ö
@app.get("/api/logs")
async def api_logs():
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ª–æ–≥–æ–≤"""
    logs = get_log_files()
    return {"logs": [log.__dict__ for log in logs]}


@app.get("/api/reports")
async def api_reports():
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –æ—Ç—á–µ—Ç–æ–≤"""
    reports = get_report_files()
    return {"reports": [report.__dict__ for report in reports]}


@app.get("/api/log/{filename}/trace")
async def api_log_trace(filename: str):
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–π—Å–∞ –ª–æ–≥–∞"""
    log_file = LOGS_DIR / filename
    
    if not log_file.exists():
        raise HTTPException(status_code=404, detail="–õ–æ–≥ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ openai_request —à–∞–≥–∞
        model_info = None
        model_name = "unknown"
        
        for step in log_data.get("log", []):
            if step.get("step_type") == "openai_request" and step.get("model_info"):
                model_info = step["model_info"]
                model_name = model_info.get("model", "unknown")
                break
        
        trace_data = {
            "task": f"Agent ID: {log_data.get('id', 'Unknown')[:20]}...",
            "total_steps": len(log_data.get("log", [])),
            "dataset": "SealQA",
            "model": model_name,
            "model_info": model_info,
            "state": log_data.get("context", {}).get("state", "unknown"),
            "github_url": "https://github.com/vakovalskii/sgr-deep-research",
            "trace": []
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
        for step in log_data.get("log", []):
            step_type = step.get("step_type", "Unknown")
            tool_name = step.get("tool_name", "")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–π—Å–∞
            if step_type == "tool_execution" and tool_name:
                # –£–±–∏—Ä–∞–µ–º 'tool' –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –¥–µ–ª–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è
                clean_name = tool_name.replace("tool", "").replace("_", " ")
                summary = clean_name.title()
            elif step_type == "openai_request":
                model = step.get("model_info", {}).get("model", "AI")
                summary = f"AI Request ({model})"
            else:
                summary = step_type.replace("_", " ").title()
            
            trace_step = {
                "step_number": step.get("step_number"),
                "step_type": step_type,
                "timestamp": step.get("timestamp"),
                "summary": summary,
                "tool_name": tool_name,
                "model_info": step.get("model_info") if step_type == "openai_request" else None
            }
            trace_data["trace"].append(trace_step)
        
        return trace_data
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ç—Ä–µ–π—Å–∞ –ª–æ–≥–∞ {filename}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –ª–æ–≥–∞")


@app.get("/api/log/{filename}/analytics")
async def api_log_analytics(filename: str):
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –ª–æ–≥—É"""
    log_file = LOGS_DIR / filename
    
    if not log_file.exists():
        raise HTTPException(status_code=404, detail="–õ–æ–≥ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        analytics = analyze_log(log_data)
        return analytics.dict()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ª–æ–≥–∞ {filename}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ª–æ–≥–∞")


@app.get("/api/report/{filename}/analytics")
async def api_report_analytics(filename: str):
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –æ—Ç—á–µ—Ç—É"""
    report_file = REPORTS_DIR / filename
    
    if not report_file.exists():
        raise HTTPException(status_code=404, detail="–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(report_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        analytics = analyze_report(content)
        return analytics.dict()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ—Ç—á–µ—Ç–∞ {filename}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ—Ç—á–µ—Ç–∞")


@app.get("/api/log/{filename}/trace")
async def api_log_trace(filename: str):
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–π—Å–∞ —à–∞–≥–æ–≤ –∞–≥–µ–Ω—Ç–∞"""
    log_file = LOGS_DIR / filename
    
    if not log_file.exists():
        raise HTTPException(status_code=404, detail="–õ–æ–≥ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        trace_steps = []
        log_entries = log_data.get('log', [])
        
        for step in log_entries:
            step_info = {
                "step_number": step.get('step_number', 0),
                "step_type": step.get('step_type', 'unknown'),
                "timestamp": step.get('timestamp', ''),
                "title": f"Step {step.get('step_number', 0)}: {step.get('step_type', '').title()}",
                "status": "completed",  # –í—Å–µ —à–∞–≥–∏ –≤ –ª–æ–≥–µ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã
                "duration": None
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –¥–ª—è —Ç–∏–ø–∞ —à–∞–≥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if step.get('step_type') == 'reasoning':
                reasoning = step.get('agent_reasoning', {})
                step_info.update({
                    "summary": reasoning.get('question_analysis', 'Reasoning step'),
                    "details": {
                        "reasoning_steps": reasoning.get('reasoning_steps', []),
                        "current_situation": reasoning.get('current_situation', ''),
                        "enough_data": reasoning.get('enough_data', False),
                        "task_completed": reasoning.get('task_completed', False)
                    }
                })
            elif step.get('step_type') == 'tool_execution':
                tool_name = step.get('tool_name', 'Unknown tool')
                step_info.update({
                    "summary": f"Executed {tool_name}",
                    "tool_name": tool_name,
                    "details": {
                        "context": step.get('agent_tool_context', {}),
                        "result_length": len(str(step.get('agent_tool_execution_result', '')))
                    }
                })
            else:
                step_info.update({
                    "summary": f"{step.get('step_type', 'Unknown')} step",
                    "details": {}
                })
            
            trace_steps.append(step_info)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–≥–∞
        model_info = log_data.get('model_info', {})
        model_name = model_info.get('model', 'gpt-4o-mini')  # Fallback –∫ gpt-4o-mini
        
        return {
            "task": log_data.get('task', 'Unknown task'),
            "agent_id": log_data.get('id', 'Unknown agent'),
            "total_steps": len(trace_steps),
            "state": log_data.get('context', {}).get('state', 'unknown'),
            "model": model_name,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –ª–æ–≥–∞
            "model_info": model_info,  # –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
            "dataset": "SealQA",  # –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
            "github_url": "https://github.com/vakovalskii/sgr-deep-research",
            "trace": trace_steps
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç—Ä–µ–π—Å–∞ –¥–ª—è –ª–æ–≥–∞ {filename}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç—Ä–µ–π—Å–∞ –ª–æ–≥–∞")


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "logs_directory": str(LOGS_DIR),
        "reports_directory": str(REPORTS_DIR),
        "logs_count": len(list(LOGS_DIR.glob("*.json")) if LOGS_DIR.exists() else []),
        "reports_count": len(list(REPORTS_DIR.glob("*.md")) if REPORTS_DIR.exists() else [])
    }


def main():
    """–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞"""
    print("üöÄ –ó–∞–ø—É—Å–∫ SGR Deep Research Log Viewer...")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ª–æ–≥–æ–≤: {LOGS_DIR}")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –æ—Ç—á–µ—Ç–æ–≤: {REPORTS_DIR}")
    print(f"üåê –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:8098")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    uvicorn.run(
        "log_viewer_app:app",
        host="0.0.0.0",
        port=8098,
        reload=True,
        log_level="info"
    )


if __name__ == "__main__":
    main()
